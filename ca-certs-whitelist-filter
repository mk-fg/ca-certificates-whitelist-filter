#!/usr/bin/env python

import itertools as it, collections as cs, hashlib as hl, pathlib as pl
import os, sys, re, shutil, fnmatch, base64

try:
	import cryptography.x509 as c_x509
	import cryptography.hazmat.primitives.hashes as c_hashes
except ImportError: c_x509 = None


p_err = lambda *a,**kw: print(*a, **kw, file=sys.stderr, flush=True)

class adict(dict):
	def __init__(self, *args, **kws):
		super().__init__(*args, **kws)
		self.__dict__ = self

def str_cut(s, max_len=80, len_bytes=False, repr_fmt=False, ext=' ...[{s_len}]'):
	'''Truncates longer strings to "max_len", adding "ext" suffix.
		Squashes line-breaks to ⏎, unless bytes or repr_fmt are used for full repr() escaping.'''
	if isinstance(s, bytes): s, repr_fmt = s.decode('utf-8', 'replace'), True
	if not isinstance(s, str): s = str(s)
	if repr_fmt: s = repr(s)[1:-1] # for logs and such, to escape all weird chars
	else: s = s.replace('\n', '⏎')
	s_len, ext_tpl = f'{len(s):,d}', ext.format(s_len='12/345')
	if max_len > 0 and len(s) > max_len:
		s_len = f'{max_len}/{s_len}'
		if not len_bytes: s = s[:max_len - len(ext_tpl)] + ext.format(s_len=s_len)
		else:
			n = max_len - len(ext_tpl.encode())
			s = s.encode()[:n].decode(errors='ignore') + ext.format(s_len=s_len)
	return s

def b64_hash(data, n=10, person=b'cacertsw', **b2_kws):
	if not isinstance(data, bytes): data = data.encode()
	digest = hl.blake2s(data, person=person, **b2_kws).digest()
	while True:
		b64 = base64.urlsafe_b64encode(digest).decode().replace('_', '').replace('-', '')
		if len(b64) >= n: return b64[:n]
		digest = hl.blake2s(digest, person=person, **b2_kws).digest()


def parse_p11k_conf(src, name=None):
	p_err_sec = lambda s: p_err(f'ERROR: {src_pre} {s}')
	sec, props, pem, pem_str = None, dict(), list(), None
	p11k_map = cs.defaultdict(list)
	for n, line in enumerate(it.chain(src, ['[end]']), 1):
		src_name = name or src.name
		src_pre = f'[ {src_name}:{n} ]'
		if re.search(r'^\s*(#|$)', line): continue
		elif m := re.search(r'^\s*\[(\S+)\]\s*$', line):
			if sec:
				obj = adict(sec=sec, props=props.copy(), pem=pem_str, conf=src_name)
				if not (label := props.get('label')):
					p_err_sec(f'Skipping {sec} without label :: {str_cut(obj)}')
				elif pem_str is None:
					p_err_sec(f'Dropping no-PEM {sec} :: {str_cut(obj)}')
				else: p11k_map[label.strip('"')].append(obj)
			sec = m[1]; props.clear(); pem.clear(); pem_str = None
			continue
		elif sec:
			if m := re.search(r'^\s*(\S+):\s+(\S.*?)\s*$', line):
				if m[1] in props: p_err_sec(f'Skipping duplicate property :: {line.strip()}')
				else: props[m[1]] = m[2]
				continue
			elif not pem and re.search(r'^--+\s*BEGIN .*--+$', line): pem.append(line); continue
			elif pem and re.search(r'^[a-zA-Z0-9+/=]+$', line): pem.append(line); continue
			elif pem and re.search(r'^--+END .*--+$', line):
				pem.append(line); pem_str = ''.join(pem); pem.clear(); continue
			p_err_sec(f'Unrecognized section line :: {line.strip()}')
		else: p_err_sec(f'Unrecognized line :: {line.strip()}')
	return p11k_map

def parse_p11k_file(p):
	with p.open() as src:
		src_hash = (m := re.fullmatch(
			r'#.*\[([a-zA-Z0-9]{10})\]', line := src.readline().strip() )) and m[1]
		if src_hash and src_hash == b64_hash(lines := src.read()):
			src_is_origin, lines = False, lines.splitlines(keepends=True)
		else: src.seek(0); src_is_origin, lines = True, src
		return parse_p11k_conf(lines, p.name), src_is_origin

def print_p11k_map(p11k_map, file=sys.stdout):
	p = lambda s='': print(s, file=file)
	for n, obj in enumerate(it.chain.from_iterable(p11k_map.values())):
		if n: p()
		p(f'[{obj.sec}]')
		for k, v in obj.props.items(): p(f'{k}: {v}')
		p(obj.pem.strip())

def print_ca_info_map(ca_info_map):
	for n, (label, objs) in enumerate(ca_info_map.items(), 1):
		print(n, label)
		if not c_x509: continue
		certs = list( (obj.pem, obj.conf)
			for obj in objs if 'BEGIN CERTIFICATE' in obj.pem )
		for pem, conf in certs:
			pre = f'{conf} ' if len(certs) > 1 else ''
			cert = c_x509.load_pem_x509_certificate(pem.encode())
			for alg in 'SHA1', 'SHA256':
				fp = cert.fingerprint(getattr(c_hashes, alg)()).hex(':').upper()
				print(f'   {pre}{alg:<6s} = {fp}')
			for a in cert.subject: print(f'   {pre}{a.oid._name}: {a.value}')
			print()


def main(argv=None):
	import argparse, textwrap
	dd = lambda text: re.sub( r' \t+', ' ',
		textwrap.dedent(text).strip('\n') + '\n' ).replace('\t', '  ')
	parser = argparse.ArgumentParser(
		formatter_class=argparse.RawTextHelpFormatter,
		usage='%(prog)s [opts] [-w whitelist-file] [trust-dir]', description=dd('''
			Trim down list of trusted CAs used by ca-certificates/p11-kit.
			Unlike built-in p11-kit tools, which use blacklist logic, this script does the
				opposite - picks couple useful CAs according to whitelist, discards all others.
			Intended to be run when contents of trust-dir change, e.g. via package manager hook.
			Tested to work with Mozilla's set of CA certs in p11-kit format.'''))

	parser.add_argument('trust_dir', nargs='?',
		default='/usr/share/ca-certificates/trust-source', help=dd('''
			ca-certificates/trust-source directory,
				where CA PEM bundles are stored in p11-kit ini-like format.
			All files in the directory will be filtered and replaced.
			Default: %(default)s'''))
	parser.add_argument('-w', '--whitelist', metavar='file', help=dd('''
		List of p11-kit object labels to include in resulting CA bundle, ignoring all others.
		One per line, case-sensitive, can use shell-glob/fnmatch wildcards.
		Must be specified for normal filtering mode, but not required for --list-all.'''))
	parser.add_argument('-q', '--quiet', action='store_true', help=dd('''
		Do not print warnings, like when patterns in a whitelist file don't match anything.'''))

	group = parser.add_argument_group('Non-essential file/dir names')
	group.add_argument('-b', '--backup-dir', default='backup', metavar='dir', help=dd('''
		Dir to copy original unprocessed file(s) to,
			just in case, and to maybe re-process them later.
		Can be relative to trust-dir, empty value to disable. Default: %(default)s'''))
	group.add_argument('-x', '--junk-files-re',
		default=r'(?i)^(README(\..*)?|\.txt)$', metavar='regexp', help=dd('''
			Regexp (python re syntax) to match name(s) of files in trust-dir
				to discard without processing, such as README and other expected junk.
			These files are moved into -b/--backup-dir or removed, with their contents ignored.
			Empty - don't skip anything. Default: %(default)s'''))

	group = parser.add_argument_group('List/print and debug options')
	group.add_argument('-p', '--print', action='store_true',
		help='Print all filtered p11-kit configuration to stdout, same as written to files.')
	group.add_argument('-L', '--list-all', action='store_true', help=dd('''
		List all p11-kit object labels and exit. Implies -n/--dry-run.
		Also prints certificate information if cryptography.io module is available.'''))
	group.add_argument('-l', '--list', action='store_true',
		help='Like -L/--list-all, but only list filtered p11-kit objects.')
	group.add_argument('-n', '--dry-run', action='store_true',
		help='Do not create or replace any files, just parse, apply all filtering and exit.')
	opts = parser.parse_args(sys.argv[1:] if argv is None else argv)

	list_mode = opts.list or opts.list_all
	dry_run = opts.dry_run or list_mode
	if not opts.list_all:
		if not opts.whitelist: parser.error('-w/--whitelist file must be specified')
		whitelist, whitelist_used = list(), set()
		for line in pl.Path(opts.whitelist).read_text().splitlines():
			if m := re.search(r'(^|\s)#', line): line = line[:m.start()]
			if pat := line.strip(): whitelist.append(pat)
	re_fn_junk = re.compile(opts.junk_files_re or '$x')

	ca_lists = list(p for p in pl.Path(opts.trust_dir).iterdir() if p.is_file())
	if opts.backup_dir:
		bak_dir = pl.Path(opts.trust_dir) / opts.backup_dir
		if not dry_run: bak_dir.mkdir(mode=0o700, exist_ok=True)
	else: bak_dir = None

	ca_info_map = cs.defaultdict(list)
	if opts.list_all:
		for p in ca_lists:
			if re_fn_junk.search(p.name): continue
			p11k_map, p_is_origin = parse_p11k_file(p)
			for label, objs in p11k_map.items(): ca_info_map[label].extend(objs)
		return print_ca_info_map(ca_info_map)

	tmp_files = list() if not dry_run else None
	try:
		for p in ca_lists:
			if re_fn_junk.search(p.name): # junk files - backed-up or removed
				if not dry_run: tmp_files.append((p, bak_dir and bak_dir / p.name))
				continue

			p11k_map, p_is_origin = parse_p11k_file(p)
			for label in list(p11k_map):
				try: pat = next(pat for pat in whitelist if fnmatch.fnmatchcase(label, pat))
				except StopIteration: del p11k_map[label]
				else: whitelist_used.add(pat)

			if list_mode:
				for label, objs in p11k_map.items(): ca_info_map[label].extend(objs)
			if opts.print:
				print('-'*20, p.name)
				print_p11k_map(p11k_map)
			if dry_run:
				with open(os.devnull, 'w') as null: print_p11k_map(p11k_map, null)
				continue

			if bak_dir and p_is_origin: # don't overwrite original backups
				shutil.copyfile(p, bak_tmp := pl.Path(
					str(bak := bak_dir / f'{p.name}') + '.old' ))
				tmp_files.append((bak_tmp, bak))
			if not p11k_map: continue # don't create empty files
			with (p_tmp := (bak_dir or p.parent) / f'{p.name}.new').open('w+') as tmp:
				tmp_files.append((p_tmp, p))
				tmp.write( line := '## Generated by'
					f' ca-certs-whitelist-filter from {p.name} [{b64_hash("")}]\n' )
				pos_hash, pos_data = line.rfind('[') + 1, tmp.tell()
				tmp.write('\n'); print_p11k_map(p11k_map, tmp)
				tmp.seek(pos_data); src_hash = b64_hash(tmp.read())
				tmp.seek(pos_hash); tmp.write(src_hash)

		if not opts.quiet:
			for pat in set(whitelist).difference(whitelist_used):
				p_err(f'WARNING: Whitelist-pattern did not match anything [ {pat} ]')
		if list_mode: print_ca_info_map(ca_info_map)
		if dry_run: return

		while tmp_files: # move all new/backup files into place
			p_tmp, p = tmp_files.pop()
			if p: p_tmp.rename(p)
			else: p_tmp.unlink(missing_ok=True)

	finally:
		for p_tmp, p in tmp_files or list(): p_tmp.unlink(missing_ok=True)

if __name__ == '__main__': sys.exit(main())
